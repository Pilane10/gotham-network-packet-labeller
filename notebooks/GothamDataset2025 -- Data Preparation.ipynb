{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGmsWHb8dUQnfgXadh988N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/othmbela/gotham-network-packet-labeller/blob/main/notebooks/GothamDataset2025%20--%20Data%20Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gotham Dataset 2025"
      ],
      "metadata": {
        "id": "QHlwDLmYvoqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the datasets"
      ],
      "metadata": {
        "id": "twuOo-Vlqx8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "QYk5V4W2qult"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9tmBe0YqBz0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0i7q5nq5q3m"
      },
      "source": [
        "### Import data from Google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8peAvfLN74Z0"
      },
      "source": [
        "The dataset can be imported using google drive. Import drive and use mount keyword to make drive as active directory. Variable basedir stores the location of folder where dataset is stored in the drive."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1kHA3iv4ojZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFHQ3XBf5zYM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OzyIveL8sP-"
      },
      "outputs": [],
      "source": [
        "# change this line your folder where the data is found\n",
        "basedir = '/content/drive/MyDrive/GothamDataset2025/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(basedir,'iotsim-air-quality-1.csv'))"
      ],
      "metadata": {
        "id": "NCgy0TimribL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNQz3_3qs5gC"
      },
      "source": [
        "## Dataset Description and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_0uOs5gtcuF"
      },
      "source": [
        "The dataset is explored more here and some information about the dataset is described. Some functions for instance .info(), .isna() etc are used. This section will include how we can manipulate the dataset to how to access specific values/attributes from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have an initial inspection of the data"
      ],
      "metadata": {
        "id": "8Fs6Y447wf7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0-H6o5yuwVZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "XGGMddf6wYSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=[int, float])"
      ],
      "metadata": {
        "id": "r6Jt4qXnwZSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=[object]).transpose()"
      ],
      "metadata": {
        "id": "CapX4p33wafa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.value_counts()"
      ],
      "metadata": {
        "id": "V_euFrYnwbM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with features with quasi null std deviation\n",
        "\n",
        "Standard deviation denoted by sigma (σ) is the average of the squared root differences from the mean."
      ],
      "metadata": {
        "id": "p3VqerhHxe-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_std = df.std(numeric_only=True)\n",
        "df_std"
      ],
      "metadata": {
        "id": "VHgwKPJdxgIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Features that meet the threshold\n",
        "constant_features = [column for column, std in df_std.items() if std < 0.01]\n",
        "\n",
        "# Drop the constant features\n",
        "df.drop(labels=constant_features, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ENIg77jNw2Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "C7PJ8F7VyCnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import (\n",
        "    FunctionTransformer,\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    LabelEncoder,\n",
        ")"
      ],
      "metadata": {
        "id": "PIw-zv_vzU1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_protocols_and_ports(df):\n",
        "    \"\"\"\n",
        "    Extract protocol and port information from packet data.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with extracted protocol and port information.\n",
        "    \"\"\"\n",
        "    src_ports, dst_ports, protocols = [], [], []\n",
        "    for _, pkt in df.iterrows():\n",
        "        if \":tcp\" in pkt[\"frame.protocols\"]:\n",
        "            protocol = \"TCP\"\n",
        "            src_port = int(pkt[\"tcp.srcport\"])\n",
        "            dst_port = int(pkt[\"tcp.dstport\"])\n",
        "        elif \":udp\" in pkt[\"frame.protocols\"]:\n",
        "            protocol = \"UDP\"\n",
        "            src_port = int(pkt[\"udp.srcport\"])\n",
        "            dst_port = int(pkt[\"udp.dstport\"])\n",
        "        elif \":icmp\" in pkt[\"frame.protocols\"]:\n",
        "            protocol = \"ICMP\"\n",
        "            src_port = np.nan\n",
        "            dst_port = np.nan\n",
        "\n",
        "        protocols.append(protocol)\n",
        "        src_ports.append(src_port)\n",
        "        dst_ports.append(dst_port)\n",
        "\n",
        "    df[\"ip.protocol\"] = protocols\n",
        "    df[\"src.port\"] = src_ports\n",
        "    df[\"dst.port\"] = dst_ports\n",
        "\n",
        "    df.drop(\n",
        "        columns=[\n",
        "            \"ip.proto\",\n",
        "            \"tcp.srcport\",\n",
        "            \"tcp.dstport\",\n",
        "            \"udp.srcport\",\n",
        "            \"udp.dstport\",\n",
        "        ],\n",
        "        axis=1,\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_time(df):\n",
        "    \"\"\"\n",
        "    Convert frame time to Unix timestamp.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with timestamp column.\n",
        "    \"\"\"\n",
        "    df[\"frame.time\"] = df[\"frame.time\"].str.replace(\"  \", \" \")\n",
        "    df[\"frame.time\"] = df[\"frame.time\"].str.replace(\" BST\", \"\")\n",
        "    df[\"frame.time\"] = pd.to_datetime(\n",
        "        df[\"frame.time\"], format=\"%b %d, %Y %H:%M:%S.%f000\"\n",
        "    )\n",
        "    df[\"timestamp\"] = df[\"frame.time\"].values.astype(np.int64) // 10**9\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_ports(df, port_hierarchy_map_iot):\n",
        "    \"\"\"\n",
        "    Convert ports to categorical values using a predefined port hierarchy.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with converted ports.\n",
        "    \"\"\"\n",
        "    def port_to_categories(port, port_hierarchy_map_iot):\n",
        "      \"\"\"Convert port number to category according to port_map.\"\"\"\n",
        "      for p_range, p_name in port_hierarchy_map_iot:\n",
        "          if port in p_range:\n",
        "              return p_name\n",
        "\n",
        "      return \"\"\n",
        "\n",
        "    df[\"src.port\"] = df[\"src.port\"].apply(lambda port: port_to_categories(port, port_hierarchy_map_iot))\n",
        "    df[\"dst.port\"] = df[\"dst.port\"].apply(lambda port: port_to_categories(port, port_hierarchy_map_iot))\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_checksums(df):\n",
        "    \"\"\"\n",
        "    Convert checksum fields to integers, replacing missing values with a default value.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with converted checksum fields.\n",
        "    \"\"\"\n",
        "    df[\"ip.checksum\"] = df[\"ip.checksum\"].apply(lambda x: int(str(x), 16) if pd.notna(x) else 0)\n",
        "    df[\"tcp.checksum\"] = df[\"tcp.checksum\"].apply(lambda x: int(str(x), 16) if pd.notna(x) else 0)\n",
        "    df[\"tcp.options\"] = df[\"tcp.options\"].apply(lambda x: int(str(x), 16) if pd.notna(x) else 0).astype(float)\n",
        "\n",
        "    return df\n",
        "\n",
        "def fill_missing_values(df):\n",
        "    \"\"\"\n",
        "    Fill missing values with a default value (-1).\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with missing values filled.\n",
        "    \"\"\"\n",
        "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
        "    df[num_cols] = df[num_cols].fillna(-1)\n",
        "\n",
        "    cat_cols = df.select_dtypes(exclude=[\"number\"]).columns\n",
        "    df[cat_cols] = df[cat_cols].fillna(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def group_labels(df):\n",
        "    \"\"\"\n",
        "    Group the attack labels into broader categories.\n",
        "    \"\"\"\n",
        "    attack_group = {\n",
        "        \"Normal\": \"Normal\",\n",
        "        \"TCP Scan\": \"Network Scanning\",\n",
        "        \"UDP Scan\": \"Network Scanning\",\n",
        "        \"Telnet Brute Force\": \"Brute Force\",\n",
        "        \"Reporting\": \"Infection\",\n",
        "        \"Ingress Tool Transfer\": \"Infection\",\n",
        "        \"File Download\": \"Infection\",\n",
        "        \"CoAP Amplification\": \"DoS\",\n",
        "        \"Merlin TCP Flooding\": \"DoS\",\n",
        "        \"Merlin UDP Flooding\": \"DoS\",\n",
        "        \"Merlin ICMP Flooding\": \"DoS\",\n",
        "        \"Merlin C&C Communication\": \"C&C Communication\",\n",
        "        \"Mirai TCP Flooding\": \"DoS\",\n",
        "        \"Mirai UDP Flooding\": \"DoS\",\n",
        "        \"Mirai GRE Flooding\": \"DoS\",\n",
        "        \"Mirai C&C Communication\": \"C&C Communication\",\n",
        "    }\n",
        "\n",
        "    # Create grouped label column\n",
        "    df[\"label_category\"] = df[\"label\"].map(lambda x: attack_group.get(x, \"Other\"))\n",
        "    return df\n",
        "\n",
        "def select_columns(df):\n",
        "    \"\"\"\n",
        "    Select relevant columns from the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataset.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataset with selected columns.\n",
        "    \"\"\"\n",
        "    selected_columns = [\n",
        "        \"timestamp\", \"frame.len\", \"ip.protocol\", \"src.port\",\n",
        "        \"dst.port\", \"ip.flags\", \"ip.ttl\", \"ip.checksum\",\n",
        "        \"tcp.flags\", \"tcp.window_size_value\", \"tcp.window_size_scalefactor\", \"tcp.checksum\",\n",
        "        \"tcp.options\", \"tcp.pdu.size\", \"label\", \"label_category\",\n",
        "    ]\n",
        "\n",
        "    df = df[selected_columns]\n",
        "    df = df[df[\"label\"] != \"Unknown\"]\n",
        "    return df\n",
        "\n",
        "def unpack_flags(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # Unpack IP flags\n",
        "    ip_flags = X[\"ip.flags\"].apply(lambda x: int(x, 16)).values.astype(np.uint8)\n",
        "    ip_flags = np.unpackbits(ip_flags.reshape((-1, 1)), axis=1, bitorder=\"little\")[:, :3]\n",
        "\n",
        "    # Unpack TCP flags (handle -1 for missing values)\n",
        "    tcp_flags = X[\"tcp.flags\"]\\\n",
        "        .apply(lambda x: int(x, 16) if x != -1 else 0)\\\n",
        "        .values.astype(np.uint8)\n",
        "\n",
        "    tcp_flags = np.unpackbits(tcp_flags.reshape((-1, 1)), axis=1, bitorder=\"little\")[:, :9]\n",
        "\n",
        "    # Combine flags into a single output array\n",
        "    return np.hstack([ip_flags, tcp_flags])"
      ],
      "metadata": {
        "id": "0sD3wutjzpgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_SIZE, TESTING_SIZE = 0.2, 0.2\n",
        "\n",
        "OUTPUT_DIR = \"\"\n",
        "\n",
        "GLOBAL_CATEGORICAL_VALUES = pd.DataFrame({\n",
        "    \"ip.protocol\": ['TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP', 'UDP', 'ICMP', 'TCP'],\n",
        "    \"src.port\": ['mqttPorts', 'coapPorts', 'rtspPorts', 'httpPorts', 'mailPorts', 'dnsPorts', 'ftpPorts', 'shellPorts', 'remoteExecPorts', 'authPorts', 'passwordPorts', 'newsPorts', 'chatPorts', 'printPorts', 'timePorts', 'dbmsPorts', 'dhcpPorts', 'whoisPorts', 'netbiosPorts', 'kerberosPorts', 'RPCPorts', 'snmpPorts', 'PRIVILEGED_PORTS', 'NONPRIVILEGED_PORTS', ''],\n",
        "    \"dst.port\": ['mqttPorts', 'coapPorts', 'rtspPorts', 'httpPorts', 'mailPorts', 'dnsPorts', 'ftpPorts', 'shellPorts', 'remoteExecPorts', 'authPorts', 'passwordPorts', 'newsPorts', 'chatPorts', 'printPorts', 'timePorts', 'dbmsPorts', 'dhcpPorts', 'whoisPorts', 'netbiosPorts', 'kerberosPorts', 'RPCPorts', 'snmpPorts', 'PRIVILEGED_PORTS', 'NONPRIVILEGED_PORTS', ''],\n",
        "})\n",
        "\n",
        "GLOBAL_LABEL_VALUES = pd.DataFrame({\n",
        "    \"label\": [\"Normal\", \"Network Scanning\", \"Brute Force\", \"Infection\", \"C&C Communication\", \"DoS\"]\n",
        "})\n",
        "\n",
        "PORT_HIERARCHY_MAP_IOT = [\n",
        "    ([1883, 8883], \"mqttPorts\"),\n",
        "    ([5683, 5684], \"coapPorts\"),\n",
        "    ([8554, 8322, 8000, 8001, 8002, 8003, 1935, 8888], \"rtspPorts\"),\n",
        "    ([80, 280, 443, 591, 593, 777, 488, 1183, 1184, 2069, 2301, 2381, 8008, 8080], \"httpPorts\"),\n",
        "    ([24, 25, 50, 58, 61, 109, 110, 143, 158, 174, 209, 220, 406, 512, 585, 993, 995], \"mailPorts\"),\n",
        "    ([42, 53, 81, 101, 105, 261], \"dnsPorts\"),\n",
        "    ([20, 21, 47, 69, 115, 152, 189, 349, 574, 662, 989, 990], \"ftpPorts\"),\n",
        "    ([22, 23, 59, 87, 89, 107, 211, 221, 222, 513, 614, 759, 992], \"shellPorts\"),\n",
        "    ([512, 514], \"remoteExecPorts\"),\n",
        "    ([13, 56, 113, 316, 353, 370, 749, 750], \"authPorts\"),\n",
        "    ([229, 464, 586, 774], \"passwordPorts\"),\n",
        "    ([114, 119, 532, 563], \"newsPorts\"),\n",
        "    ([194, 258, 531, 994], \"chatPorts\"),\n",
        "    ([35, 92, 170, 515, 631], \"printPorts\"),\n",
        "    ([13, 37, 52, 123, 519, 525], \"timePorts\"),\n",
        "    ([65, 66, 118, 150, 156, 217], \"dbmsPorts\"),\n",
        "    ([546, 547, 647, 847], \"dhcpPorts\"),\n",
        "    ([43, 63], \"whoisPorts\"),\n",
        "    (range(137, 139 + 1), \"netbiosPorts\"),\n",
        "    ([88, 748, 750], \"kerberosPorts\"),\n",
        "    ([111, 121, 369, 530, 567, 593, 602], \"RPCPorts\"),\n",
        "    ([161, 162, 391], \"snmpPorts\"),\n",
        "    (range(0, 1024), \"PRIVILEGED_PORTS\"),\n",
        "    (range(1024, 65536), \"NONPRIVILEGED_PORTS\")\n",
        "]"
      ],
      "metadata": {
        "id": "9CcR_QOrYtPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = glob.glob(os.path.join(basedir, \"*.csv\"))\n",
        "\n",
        "for filename in filenames:\n",
        "\n",
        "    print(f\"Processing {filename}\")\n",
        "\n",
        "    processed_chunks = []  # List to hold processed data chunks\n",
        "\n",
        "    # Process CSV file in chunks for memory efficiency\n",
        "    for chunk in pd.read_csv(filename, sep=\",\", low_memory=False, chunksize=10000):\n",
        "        chunk = extract_protocols_and_ports(chunk)\n",
        "        chunk = convert_time(chunk)\n",
        "        chunk = convert_ports(chunk, PORT_HIERARCHY_MAP_IOT)\n",
        "        chunk = convert_checksums(chunk)\n",
        "        chunk = fill_missing_values(chunk)\n",
        "        chunk = group_labels(chunk)\n",
        "        chunk = select_columns(chunk)\n",
        "\n",
        "        processed_chunks.append(chunk)\n",
        "\n",
        "    # Concatenate all processed chunks into a single DataFrame\n",
        "    df = pd.concat(processed_chunks, ignore_index=True)\n",
        "\n",
        "    # Split data into training, validation, and testing sets\n",
        "\n",
        "    labels = df[\"label_category\"]\n",
        "    features = df.drop(labels=[\"label\", \"label_category\"], axis=1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features,\n",
        "        labels,\n",
        "        test_size=(VALIDATION_SIZE + TESTING_SIZE),\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "    X_test, X_val, y_test, y_val = train_test_split(\n",
        "        X_test,\n",
        "        y_test,\n",
        "        test_size=TESTING_SIZE / (VALIDATION_SIZE + TESTING_SIZE),\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Scale the data and split it into features and labels\n",
        "    flags_features = [\"ip.flags\", \"tcp.flags\"]\n",
        "    categorical_features = [\"ip.protocol\", \"src.port\", \"dst.port\"]\n",
        "    numeric_features = ['timestamp', 'frame.len', 'ip.ttl', 'ip.checksum',\n",
        "                        'tcp.window_size_value', 'tcp.window_size_scalefactor', 'tcp.checksum',\n",
        "                        'tcp.options', 'tcp.pdu.size']\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"flags\", FunctionTransformer(unpack_flags), flags_features),\n",
        "            (\"categoricals\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"error\"), categorical_features),\n",
        "            (\"numericals\", StandardScaler(), numeric_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preprocessor.fit(X_train)\n",
        "    preprocessor[\"categoricals\"].fit(GLOBAL_CATEGORICAL_VALUES)\n",
        "\n",
        "    # Preprocess the features\n",
        "    X_train = pd.DataFrame(preprocessor.transform(X_train))\n",
        "    X_val = pd.DataFrame(preprocessor.transform(X_val))\n",
        "    X_test = pd.DataFrame(preprocessor.transform(X_test))\n",
        "    print(X_train.shape)\n",
        "\n",
        "    # Preprocess the labels\n",
        "    le = LabelEncoder()\n",
        "    le.fit(GLOBAL_LABEL_VALUES)\n",
        "\n",
        "    y_train = pd.DataFrame(le.transform(y_train), columns=[\"label\"])\n",
        "    y_val = pd.DataFrame(le.transform(y_val), columns=[\"label\"])\n",
        "    y_test = pd.DataFrame(le.transform(y_test), columns=[\"label\"])\n",
        "\n",
        "    # Extract IoT device name from filename for saving\n",
        "    iot_device = os.path.basename(filename).rstrip(\".csv\")\n",
        "\n",
        "    # Save the processed data to pickle files for each IoT device\n",
        "    X_train.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_train_features.pkl\"), compression=\"gzip\")\n",
        "    y_train.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_train_labels.pkl\"), compression=\"gzip\",)\n",
        "\n",
        "    X_val.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_val_features.pkl\"), compression=\"gzip\")\n",
        "    y_val.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_val_labels.pkl\"), compression=\"gzip\")\n",
        "\n",
        "    X_test.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_test_features.pkl\"), compression=\"gzip\")\n",
        "    y_test.to_pickle(os.path.join(basedir, \"processed\", f\"{iot_device}_test_labels.pkl\"), compression=\"gzip\")\n",
        "\n",
        "    # Clean up memory by deleting intermediate variables and performing garbage collection\n",
        "    del (\n",
        "        chunk,\n",
        "        # df,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        X_test,\n",
        "        y_test,\n",
        "    )\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "Ncbt2RHnyHSw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}