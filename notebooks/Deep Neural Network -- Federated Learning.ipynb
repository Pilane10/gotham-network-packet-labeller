{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "view-in-github",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/othmbela/gotham-network-packet-labeller/blob/main/notebooks/Deep%20Neural%20Network%20--%20Federated%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4e7f5",
   "metadata": {
    "id": "e3f4e7f5"
   },
   "source": [
    "# Gotham Dataset 2025: Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a103495",
   "metadata": {
    "id": "4a103495"
   },
   "source": [
    "Welcome to this tutorial on using the Smart Cities Network Security Dataset for training a deep learning model. In this notebook, we extend our model to a federated learning (FL) setup, where training is distributed across multiple smart city IoT nodes. Instead of sharing raw data, only model updates (gradients) are exchanged, preserving privacy and reducing bandwidth usage.\n",
    "\n",
    "For federated learning, the workflow remains similar but differs in how the model is trained. Instead of a single centralized dataset, data is distributed across multiple IoT devices (clients), and training occurs locally before aggregating updates on a central server. We’ll use Flower (FL), a popular framework for federated learning with PyTorch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd85b086",
   "metadata": {
    "id": "bd85b086"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Now let's really begin with this tutorial!\n",
    "\n",
    "To start working, very little is required once you have activated your Python environment (e.g. via `conda`, `virtualenv`, `pyenv`, etc). If you are running this code on Colab, there is really nothing to do except to install Flower and other dependencies. Let's first, install Flower, then the ML framework of your choice and extra dependencies you might want to use.\n",
    "\n",
    "## Installing Flower\n",
    "\n",
    "You can install flower very conveniently from `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81450a2f",
   "metadata": {
    "id": "81450a2f"
   },
   "outputs": [],
   "source": [
    "# if you want to install libraries, use\n",
    "# !pip install package_name\n",
    "!pip install -q \"flwr[simulation]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b0f5d",
   "metadata": {
    "id": "6d9b0f5d"
   },
   "source": [
    "We will be using the _simulation_ engine in Flower, which allows you to run a large number of clients without the overheads of manually managing devices. This is achieved via the `Simulation Engine`, the core component in Flower to run simulations efficiently.\n",
    "\n",
    "Now, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888849a9",
   "metadata": {
    "id": "888849a9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5ccf5",
   "metadata": {
    "id": "afc5ccf5"
   },
   "source": [
    "### Import data from Google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612168a5",
   "metadata": {
    "id": "612168a5"
   },
   "source": [
    "The dataset can be imported using google drive. Import drive and use mount keyword to make drive as active directory. Variable basedir stores the location of folder where dataset is stored in the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfaae1b",
   "metadata": {
    "id": "1bfaae1b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629914a",
   "metadata": {
    "id": "b629914a"
   },
   "outputs": [],
   "source": [
    "# change this line your folder where the data is found\n",
    "DATA_DIR = '/content/drive/MyDrive/GothamDataset2025/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6898de",
   "metadata": {
    "id": "0a6898de"
   },
   "source": [
    "## Load the Partitions\n",
    "\n",
    "To start designing a Federated Learning pipeline we need to meet one of the key properties in FL: each client has its own data partition.\n",
    "\n",
    "### A dataset\n",
    "\n",
    "Let's begin by constructing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14686e66",
   "metadata": {
    "id": "14686e66"
   },
   "outputs": [],
   "source": [
    "class GothamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features_file, target_file, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features_file (string): Path to the csv file with features.\n",
    "            target_file (string): Path to the csv file with labels.\n",
    "            transform (callable, optional): Optional transform to be applied on features.\n",
    "            target_transform (callable, optional): Optional transform to be applied on labels.\n",
    "        \"\"\"\n",
    "        self.features = pd.read_pickle(features_file, compression=\"gzip\")\n",
    "        self.labels = pd.read_pickle(target_file, compression=\"gzip\")\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features.iloc[idx, :]\n",
    "        label = self.labels.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature.values, dtype=torch.float32)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label, dtype=torch.int64)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f50673",
   "metadata": {
    "id": "d9f50673"
   },
   "outputs": [],
   "source": [
    "def load_partition(data_path: str, partition_id: int, batch_size: int):\n",
    "    \"\"\"Load training, validation and test set.\"\"\"\n",
    "\n",
    "    iot_devices = []\n",
    "    for path in glob.glob(f\"{DATA_DIR}/*_train_features.pkl\"):\n",
    "        match = re.search(r\"([^/]+)_train_features\\.pkl$\", path)\n",
    "        if match:\n",
    "            iot_devices.append(match.group(1))\n",
    "\n",
    "\n",
    "    iot_device = iot_devices[partition_id]\n",
    "\n",
    "    # Get the datasets\n",
    "    train_data = GothamDataset(\n",
    "        features_file=f\"{data_path}/{iot_device}_train_features.pkl\",\n",
    "        target_file=f\"{data_path}/{iot_device}_train_labels.pkl\",\n",
    "        transform=torch.tensor,\n",
    "        target_transform=torch.tensor\n",
    "    )\n",
    "\n",
    "    test_data = GothamDataset(\n",
    "        features_file=f\"{data_path}/{iot_device}_test_features.pkl\",\n",
    "        target_file=f\"{data_path}/{iot_device}_test_labels.pkl\",\n",
    "        transform=torch.tensor,\n",
    "        target_transform=torch.tensor\n",
    "    )\n",
    "\n",
    "    # Create the dataloaders - for training, validation and testing\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69332e9",
   "metadata": {
    "id": "e69332e9"
   },
   "source": [
    "### A DNN architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56fd5d",
   "metadata": {
    "id": "aa56fd5d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, num_features=70, hidden1_size=128, hidden2_size=128, hidden3_size=64, num_classes=6):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden3_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6fd7b",
   "metadata": {
    "id": "0ff6fd7b"
   },
   "source": [
    "Similarly to what we did with the dataset you could inspect the model in various ways. We can, for instance, count the number of model parameters.\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126d9f",
   "metadata": {
    "id": "b0126d9f"
   },
   "outputs": [],
   "source": [
    "# Defining some input variables\n",
    "n_features = 70\n",
    "n_classes = 6\n",
    "num_epochs = 3\n",
    "\n",
    "# Creating a DBN\n",
    "model = DNN(num_features=n_features,\n",
    "            hidden1_size=128,\n",
    "            hidden2_size=128,\n",
    "            hidden3_size=64,\n",
    "            num_classes=n_classes,\n",
    "            )\n",
    "num_parameters = sum(value.numel() for value in model.state_dict().values())\n",
    "print(f\"{num_parameters = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11964765",
   "metadata": {
    "id": "11964765"
   },
   "source": [
    "## Federated Learning with Flower\n",
    "\n",
    "\n",
    "Next, we simulate a scenario where:\n",
    "- Multiple datasets are distributed across multiple organizations.\n",
    "- The model is trained collaboratively across these organizations using federated learning.\n",
    "\n",
    "In federated learning, the process works as follows:\n",
    "\n",
    "- The server sends the global model parameters to the client.\n",
    "- The client:\n",
    "    - Updates the local model with the parameters received from the server.\n",
    "    - Trains the model on its local data, which changes the model parameters locally.\n",
    "    - Sends the updated/changed model parameters back to the server (or, alternatively, just the gradients).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vYMR3MumBGD1",
   "metadata": {
    "id": "vYMR3MumBGD1"
   },
   "source": [
    "### - `Train()` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jmTwOX_YFsoo",
   "metadata": {
    "id": "jmTwOX_YFsoo"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Train the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.Module\n",
    "        Neural network model used in this example.\n",
    "\n",
    "    optimizer: torch.optim\n",
    "        Optimizer.\n",
    "\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "        DataLoader used in training.\n",
    "\n",
    "    num_epochs: int\n",
    "        Number of epochs to run in each round.\n",
    "\n",
    "    device: torch.device\n",
    "        (Default value = torch.device(\"cpu\"))\n",
    "        Device where the network will be trained within a client.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}:\")\n",
    "        for i, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Passing the batch down the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # performs the gradient update\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2F7YLZmhBPd9",
   "metadata": {
    "id": "2F7YLZmhBPd9"
   },
   "source": [
    "### - `Test()` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eSlx_n8Fu6o",
   "metadata": {
    "id": "2eSlx_n8Fu6o"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Validate the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.ModuleList\n",
    "        Neural network model used in this example.\n",
    "\n",
    "    test_loader: torch.utils.data.DataLoader\n",
    "        DataLoader used in testing.\n",
    "\n",
    "    device: torch.device\n",
    "        (Default value = torch.device(\"cpu\"))\n",
    "        Device where the network will be trained within a client.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple containing the history, and a detailed report.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    history = {\n",
    "      'total': 0,\n",
    "      'loss': 0.0,\n",
    "      'accuracy': 0.0,\n",
    "      'output_pred': [],\n",
    "      'output_true': [],\n",
    "    }\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_steps = 0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    test_output_pred = []\n",
    "    test_output_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.cpu().item()\n",
    "            test_steps += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_output_pred += outputs.argmax(1).cpu().tolist()\n",
    "            test_output_true += labels.tolist()\n",
    "\n",
    "    history['loss'] = test_loss/test_steps\n",
    "    history['accuracy'] = test_correct/test_total\n",
    "    history['output_pred'] = test_output_pred\n",
    "    history['output_true'] = test_output_true\n",
    "\n",
    "    print(f\"Test loss: {history['loss']}, Test accuracy: {history['accuracy']}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc751b",
   "metadata": {
    "id": "d0bc751b"
   },
   "source": [
    "### Define a Federated Learning Client\n",
    "\n",
    "Each IoT device (client) will perform local training and send model updates to the server.\n",
    "\n",
    "Each client:\n",
    "- Loads its local dataset\n",
    "- Trains the model locally\n",
    "- Sends model updates to the central server\n",
    "\n",
    "A Flower Client is a simple Python class with two distinct methods: `fit()` and `evaluate()`. This class will be then wrapped into a ClientApp that can be used to launch the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4308183",
   "metadata": {
    "id": "e4308183"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from flwr.common import NDArrays, Scalar, Context\n",
    "from flwr.client import Client, ClientApp, NumPyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbd3e8",
   "metadata": {
    "id": "61cbd3e8"
   },
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, model, criterion, num_epochs, train_loader, test_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.num_epochs = num_epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"This method trains the model using the parameters sent by the\n",
    "        server on the dataset of this client. At then end, the parameters\n",
    "        of the locally trained model are communicated back to the server\"\"\"\n",
    "\n",
    "        # copy parameters sent by the server into client's local model\n",
    "        set_params(self.model, parameters)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # do local training (call same function as centralised setting)\n",
    "        train(self.model, self.criterion, optimizer, self.train_loader, self.num_epochs, self.device)\n",
    "\n",
    "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
    "        return get_params(self.model), len(self.train_loader), {}\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        \"\"\"Evaluate the model sent by the server on this client's\n",
    "        local validation set. Then return performance metrics.\"\"\"\n",
    "\n",
    "        set_params(self.model, parameters)\n",
    "        # do local evaluation (call same function as centralised setting)\n",
    "        history = test(self.model, self.criterion, self.test_loader, self.device)\n",
    "        loss, accuracy = history['loss'], history['accuracy']\n",
    "        # send statistics back to the server\n",
    "        return float(loss), len(self.test_loader), {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Two auxhiliary functions to set and extract parameters of a model\n",
    "def set_params(model, parameters):\n",
    "    \"\"\"Replace model parameters with those passed as `parameters`.\"\"\"\n",
    "\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(v) for k, v in params_dict})\n",
    "    # now replace the parameters\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_params(model):\n",
    "    \"\"\"Extract model parameters as a list of NumPy arrays.\"\"\"\n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75210f25",
   "metadata": {
    "id": "75210f25"
   },
   "source": [
    "#### The `client_fn` callback\n",
    "\n",
    "Now, we need to construct a `ClientApp`. This can be conveniently be done by means of a `client_fn` callback that will return a `FlowerClient` that uses a specific data partition (`partition-id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcb06a",
   "metadata": {
    "id": "bffcb06a"
   },
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DNN().to(device)\n",
    "\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load data\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data partition\n",
    "    # Read the node_config to fetch data partition associated to this node\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    train_loader, test_loader = load_partition(data_path=DATA_DIR, partition_id=partition_id, batch_size=batch_size)\n",
    "\n",
    "    # Create a single Flower client representing a single organization\n",
    "    # FlowerClient is a subclass of NumPyClient, so we need to call .to_client()\n",
    "    # to convert it to a subclass of `flwr.client.Client`\n",
    "    return FlowerClient(model, criterion, num_epochs, train_loader, test_loader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client_app = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba01f36",
   "metadata": {
    "id": "4ba01f36"
   },
   "source": [
    "### Define the Flower ServerApp\n",
    "\n",
    "The server coordinates training across IoT clients by aggregating model updates. A strategy sits at the core of the Federated Learning experiment. For this tutorial, let's use what is arguable the most popular strategy out there: FedAvg.\n",
    "\n",
    "\n",
    "FedAvg is a good strategy to start your experimentation. The way it works is simple. FedAvg, as its name implies, derives a new version of the global model by taking the average of all the models sent by clients participating in the round.\n",
    "\n",
    "This function is designed to be used with Flower's `evaluate_metrics_aggregation_fn` callback. It is executed at the end of each \"evaluate\" round, where clients evaluate the global model\n",
    "on their local data and report the results (e.g., accuracy, loss) to the server.  This function specifically computes the weighted average of the 'accuracy' metric returned  by each `FlowerClient`'s `evaluate()` method, giving more weight to clients with more data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-307O7ErCHCw",
   "metadata": {
    "id": "-307O7ErCHCw"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from flwr.common import Metrics\n",
    "\n",
    "\n",
    "# Define metric aggregation function\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4T-IrJOuCNs5",
   "metadata": {
    "id": "4T-IrJOuCNs5"
   },
   "source": [
    "We'll use this callback when defining the strategy in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d2e9",
   "metadata": {
    "id": "b877d2e9"
   },
   "source": [
    "<!-- We'll use this callback when defining the strategy in the next section. -->\n",
    "\n",
    "##### The server_fn callback\n",
    "\n",
    "The easiest way to create a ServerApp is by means of a server_fn callback. It has a similar signature to client_fn but, instead of returning a client object, it returns all the components needed to run the server-side logic in Flower. In this tutorial we'll keep things simple and stick to FedAvg with initialised global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f7f1b",
   "metadata": {
    "id": "3f9f7f1b"
   },
   "outputs": [],
   "source": [
    "from flwr.common import ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5b5a7",
   "metadata": {
    "id": "e7e5b5a7"
   },
   "outputs": [],
   "source": [
    "def server_fn(context: Context):\n",
    "\n",
    "    # instantiate the model\n",
    "    model = DNN()\n",
    "    ndarrays = get_params(model)\n",
    "    # Convert model parameters to flwr.common.Parameters\n",
    "    global_model_init = ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    # Define the strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=0.1,  # 10% clients sampled each round to do fit()\n",
    "        fraction_evaluate=0.5,  # 50% clients sample each round to do evaluate()\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # callback defined earlier\n",
    "        initial_parameters=global_model_init,\n",
    "    )\n",
    "\n",
    "    num_rounds = 5\n",
    "\n",
    "    # Construct ServerConfig\n",
    "    config = ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    # Wrap everything into a `ServerAppComponents` object\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create your ServerApp\n",
    "server_app = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560d614",
   "metadata": {
    "id": "5560d614"
   },
   "source": [
    "### Resource Configuration\n",
    "\n",
    "To control resource usage, we use the backend_config dictionary, which includes the client_resources key. This key specifies the amount of CPU and GPU resources each client can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda09f40",
   "metadata": {
    "id": "cda09f40"
   },
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "# backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "# if DEVICE.type == \"cuda\":\n",
    "    # backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11144",
   "metadata": {
    "id": "2da11144"
   },
   "source": [
    "### Launching the Simulation\n",
    "\n",
    "With both ClientApp and ServerApp ready, we can launch the simulation. The `run_simulation` function is used to execute the simulation. This function orchestrates the federated learning simulation by bringing together the `ServerApp`, `ClientApp`, and other configuration details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f72ad",
   "metadata": {
    "id": "ca3f72ad"
   },
   "outputs": [],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "\n",
    "num_clients = 2\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=num_clients,\n",
    "    # backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6fce2",
   "metadata": {
    "id": "16f6fce2"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e410291",
   "metadata": {
    "id": "3e410291"
   },
   "source": [
    "The classification report provides detailed metrics for each class, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03849cc",
   "metadata": {
    "id": "b03849cc"
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report\", end=\"\\n\\n\")\n",
    "print(classification_report(test_output_true, test_output_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1103a6",
   "metadata": {
    "id": "ae1103a6"
   },
   "source": [
    "A confusion matrix helps us understand the types of misclassifications made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255290e7",
   "metadata": {
    "id": "255290e7"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=test_output_true,\n",
    "                      y_pred=test_output_pred,\n",
    "                      labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062a79e",
   "metadata": {
    "id": "6062a79e"
   },
   "source": [
    "This analysis helps identify areas where the model may need improvement, such as handling class imbalances or misclassifications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vYMR3MumBGD1",
    "75210f25"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
