{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/othmbela/gotham-network-packet-labeller/blob/main/notebooks/Deep%20Neural%20Network%20--%20Centralised%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df54021a",
      "metadata": {
        "id": "df54021a"
      },
      "source": [
        "# Gotham Dataset 2025: Centralised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca86c1a0",
      "metadata": {
        "id": "ca86c1a0"
      },
      "source": [
        "Welcome to this tutorial on using the Smart Cities Network Security Dataset for training a deep learning model. This notebook provides a step-by-step guide to loading the dataset, preprocessing it, and training a deep neural network (DNN) using PyTorch.\n",
        "\n",
        "This dataset is designed to aid researchers in developing intrusion detection systems (IDS) for smart city environments, focusing on real-world IoT traffic characteristics.\n",
        "\n",
        "**Objectives**:\n",
        "1. Load the dataset.\n",
        "2. Define a deep learning model using PyTorch.\n",
        "3. Train and evaluate the model.\n",
        "4. Save the trained model for future use.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c463baff",
      "metadata": {
        "id": "c463baff"
      },
      "source": [
        "## Setup & Installation\n",
        "\n",
        "Now let's really begin with this tutorial!\n",
        "\n",
        "To start working, very little is required once you have activated your Python environment (e.g. via `conda`, `virtualenv`, `pyenv`, etc). If you are running this code on Colab, there is really nothing to do except to install ???. The steps below have been verified to run in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea963b4e",
      "metadata": {
        "id": "ea963b4e"
      },
      "outputs": [],
      "source": [
        "# if you want to install libraries, use\n",
        "# !pip install package_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6150eb84",
      "metadata": {
        "id": "6150eb84"
      },
      "source": [
        "Now, import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c352eacd",
      "metadata": {
        "id": "c352eacd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e9a363",
      "metadata": {
        "id": "34e9a363"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "The dataset contains labeled network traffic data collected from smart city IoT devices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b3cafc",
      "metadata": {
        "id": "d6b3cafc"
      },
      "source": [
        "The dataset can be imported using google drive. Import drive and use mount keyword to make drive as active directory. Variable basedir stores the location of folder where dataset is stored in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4331fcf",
      "metadata": {
        "id": "b4331fcf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445a4365",
      "metadata": {
        "id": "445a4365"
      },
      "outputs": [],
      "source": [
        "# change this line your folder where the data is found\n",
        "DATA_DIR = '/content/drive/MyDrive/GothamDataset2025/processed'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4bd6af",
      "metadata": {
        "id": "1d4bd6af"
      },
      "source": [
        "### Convert Dataset into PyTorch Tensor Format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeef9553",
      "metadata": {
        "id": "eeef9553"
      },
      "source": [
        "Now, we convert the dataset into a PyTorch-compatible format by creating a custom Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8623732",
      "metadata": {
        "id": "c8623732"
      },
      "outputs": [],
      "source": [
        "class GothamDataset(Dataset):\n",
        "\n",
        "    def __init__(self, features_file, target_file, transform=None, target_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features_file (string): Path to the csv file with features.\n",
        "            target_file (string): Path to the csv file with labels.\n",
        "            transform (callable, optional): Optional transform to be applied on features.\n",
        "            target_transform (callable, optional): Optional transform to be applied on labels.\n",
        "        \"\"\"\n",
        "        self.features = pd.read_pickle(features_file, compression=\"gzip\")\n",
        "        self.labels = pd.read_pickle(target_file, compression=\"gzip\")\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features.iloc[idx, :]\n",
        "        label = self.labels.iloc[idx, 0]\n",
        "        if self.transform:\n",
        "            feature = self.transform(feature.values, dtype=torch.float32)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label, dtype=torch.int64)\n",
        "        return feature, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd745c3",
      "metadata": {
        "id": "2bd745c3"
      },
      "source": [
        "###  Use DataLoader for Efficient Batch Processing\n",
        "\n",
        "Instead of loading the entire dataset at once, we use PyTorch’s DataLoader to efficiently handle batches during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3e8fbe",
      "metadata": {
        "id": "3e3e8fbe"
      },
      "outputs": [],
      "source": [
        "def get_dataset(data_path: str):\n",
        "    \"\"\"Load training, validation and test set.\"\"\"\n",
        "\n",
        "    train_datasets, val_datasets, test_datasets = [], [], []\n",
        "\n",
        "    iot_devices = []\n",
        "    for path in glob.glob(f\"{DATA_DIR}/*_train_features.pkl\"):\n",
        "        match = re.search(r\"([^/]+)_train_features\\.pkl$\", path)\n",
        "        if match:\n",
        "            iot_devices.append(match.group(1))\n",
        "\n",
        "    # Get the datasets\n",
        "    for iot_device in iot_devices:\n",
        "        train_datasets.append(GothamDataset(\n",
        "            features_file=f\"{data_path}/{iot_device}_train_features.pkl\",\n",
        "            target_file=f\"{data_path}/{iot_device}_train_labels.pkl\",\n",
        "            transform=torch.tensor,\n",
        "            target_transform=torch.tensor\n",
        "        ))\n",
        "        val_datasets.append(GothamDataset(\n",
        "            features_file=f\"{data_path}/{iot_device}_val_features.pkl\",\n",
        "            target_file=f\"{data_path}/{iot_device}_val_labels.pkl\",\n",
        "            transform=torch.tensor,\n",
        "            target_transform=torch.tensor\n",
        "        ))\n",
        "        test_datasets.append(GothamDataset(\n",
        "            features_file=f\"{data_path}/{iot_device}_test_features.pkl\",\n",
        "            target_file=f\"{data_path}/{iot_device}_test_labels.pkl\",\n",
        "            transform=torch.tensor,\n",
        "            target_transform=torch.tensor\n",
        "        ))\n",
        "\n",
        "    train_data = torch.utils.data.ConcatDataset(train_datasets)\n",
        "    val_data = torch.utils.data.ConcatDataset(val_datasets)\n",
        "    test_data = torch.utils.data.ConcatDataset(test_datasets)\n",
        "\n",
        "    return train_data, val_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93ba118",
      "metadata": {
        "id": "e93ba118"
      },
      "outputs": [],
      "source": [
        "# Get the datasets\n",
        "train_data, _, test_data = get_dataset(data_path=DATA_DIR)\n",
        "\n",
        "# How many instances have we got?\n",
        "print('# instances in training set: ', len(train_data))\n",
        "print('# instances in testing set: ', len(test_data))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# Create the dataloaders - for training, validation and testing\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb5b39d",
      "metadata": {
        "id": "ebb5b39d"
      },
      "source": [
        "## Defining the Deep Neural Network\n",
        "\n",
        "This tutorial is not so much about novel architectural designs so we define a simple deep learning model using PyTorch’s nn.Module. The model consists of multiple fully connected layers with ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c7e818",
      "metadata": {
        "id": "94c7e818"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, hidden1_size, hidden2_size, hidden3_size, num_classes):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, hidden1_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden2_size, hidden3_size)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden3_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b5ba17",
      "metadata": {
        "id": "f2b5ba17"
      },
      "source": [
        "Similarly to what we did with the dataset you could inspect the model in various ways. We can, for instance, count the number of model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba33f9b1",
      "metadata": {
        "id": "ba33f9b1"
      },
      "outputs": [],
      "source": [
        "# Defining some input variables\n",
        "n_features = 70\n",
        "n_classes = 6\n",
        "\n",
        "# Creating a DBN\n",
        "model = DNN(num_features=n_features,\n",
        "            hidden1_size=128,\n",
        "            hidden2_size=128,\n",
        "            hidden3_size=64,\n",
        "            num_classes=n_classes,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ab16ec",
      "metadata": {
        "id": "91ab16ec"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b6b85e",
      "metadata": {
        "id": "f2b6b85e"
      },
      "outputs": [],
      "source": [
        "num_parameters = sum(value.numel() for value in model.state_dict().values())\n",
        "print(f\"{num_parameters = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a892739",
      "metadata": {
        "id": "5a892739"
      },
      "source": [
        "## The Training Loop\n",
        "\n",
        "A minimal training loop in PyTorch can be constructed with three functions:\n",
        "*  `train()` that will train the model given a dataloader.\n",
        "* `test()` that will be used to evaluate the performance of the model on held-out data, e.g., a training set.\n",
        "\n",
        "Let's construct these functions!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18347bda",
      "metadata": {
        "id": "18347bda"
      },
      "source": [
        "### - `Train()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e790bde",
      "metadata": {
        "id": "1e790bde"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    criterion: torch.nn.Module,\n",
        "    optimizer: torch.optim,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    num_epochs: int,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"Train the network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Module\n",
        "        Neural network model used in this example.\n",
        "\n",
        "    optimizer: torch.optim\n",
        "        Optimizer.\n",
        "\n",
        "    train_loader: torch.utils.data.DataLoader\n",
        "        DataLoader used in training.\n",
        "\n",
        "    num_epochs: int\n",
        "        Number of epochs to run in each round.\n",
        "\n",
        "    device: torch.device\n",
        "        (Default value = torch.device(\"cpu\"))\n",
        "        Device where the network will be trained within a client.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}:\")\n",
        "        for i, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Passing the batch down the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # performs the gradient update\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"{tag} Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e9c01a",
      "metadata": {
        "id": "52e9c01a"
      },
      "source": [
        "### - `Test()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a548e87",
      "metadata": {
        "id": "2a548e87"
      },
      "outputs": [],
      "source": [
        "def test(\n",
        "    model: torch.nn.Module,\n",
        "    criterion: torch.nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"Validate the network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.ModuleList\n",
        "        Neural network model used in this example.\n",
        "\n",
        "    test_loader: torch.utils.data.DataLoader\n",
        "        DataLoader used in testing.\n",
        "\n",
        "    device: torch.device\n",
        "        (Default value = torch.device(\"cpu\"))\n",
        "        Device where the network will be trained within a client.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Tuple containing the history, and a detailed report.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_steps = 0\n",
        "    test_total = 0\n",
        "    test_correct = 0\n",
        "\n",
        "    test_output_pred = []\n",
        "    test_output_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (inputs, labels) in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.cpu().item()\n",
        "            test_steps += 1\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            test_output_pred += outputs.argmax(1).cpu().tolist()\n",
        "            test_output_true += labels.tolist()\n",
        "\n",
        "    history['loss'] = test_loss/test_steps\n",
        "    history['accuracy'] = test_correct/test_total\n",
        "    history['output_pred'] = test_output_pred\n",
        "    history['output_true'] = test_output_true\n",
        "\n",
        "    print(f'Test loss: {history['loss']}, Test accuracy: {history['accuracy']}')\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892c2315",
      "metadata": {
        "id": "892c2315"
      },
      "source": [
        "Let's run this for 5 epochs (you'll see it reaching close to 99% accuracy -- as expected from a centralised setup with the MNIST dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04646822",
      "metadata": {
        "id": "04646822"
      },
      "outputs": [],
      "source": [
        "# Discover device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimzer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Create the criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train for the specified number of epochs\n",
        "train(model, criterion, optimizer, train_loader, num_epochs, device)\n",
        "\n",
        "# Training is completed, then evaluate model on the test set\n",
        "history = test(model, criterion, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab621423",
      "metadata": {
        "id": "ab621423"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5696eb14",
      "metadata": {
        "id": "5696eb14"
      },
      "source": [
        "The classification report provides detailed metrics for each class, including precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693f2357",
      "metadata": {
        "id": "693f2357"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report\", end=\"\\n\\n\")\n",
        "print(classification_report(test_output_true, test_output_pred, target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbf4f48",
      "metadata": {
        "id": "ecbf4f48"
      },
      "source": [
        "A confusion matrix helps us understand the types of misclassifications made by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a05db8",
      "metadata": {
        "id": "e2a05db8"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_true=test_output_true,\n",
        "                      y_pred=test_output_pred,\n",
        "                      labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0c85ed",
      "metadata": {
        "id": "dd0c85ed"
      },
      "source": [
        "This analysis helps identify areas where the model may need improvement, such as handling class imbalances or misclassifications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487d3c22",
      "metadata": {
        "id": "487d3c22"
      },
      "source": [
        "### Save Model\n",
        "\n",
        "Saving the trained model allows it to be used later for real-world deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acdc5da8",
      "metadata": {
        "id": "acdc5da8"
      },
      "outputs": [],
      "source": [
        "path = '../../checkpoints/deep_neural_network.pt'\n",
        "torch.save({\n",
        "            'epoch': num_epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            }, path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}