{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f567328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc155b4",
   "metadata": {},
   "source": [
    "## Read PCAP files, Extract Network Features and Save to CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25dd3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAPReader:\n",
    "\n",
    "    def __init__(self, pcap_path, feature_vector, tool, tshark_path, zeek_path):\n",
    "        \"\"\"\n",
    "        Initializes the feature extraction process.\n",
    "\n",
    "        Parameters:\n",
    "            pcap_path (str): Path to the input pcap file.\n",
    "            features (list): List of features/fields to extract.\n",
    "            tshark_path (str): Path to the tshark executable (default is 'tshark').\n",
    "        \"\"\"\n",
    "        self.pcap_path = pcap_path\n",
    "        self.feature_vector = feature_vector\n",
    "        self.tool = tool\n",
    "        self.tshark_path = tshark_path\n",
    "        self.zeek_path = zeek_path\n",
    "        self.dataframe = None  # Will store the extracted DataFrame\n",
    "        \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Extracts features from the pcap file and returns them as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the extracted features.\n",
    "        \"\"\"\n",
    "        fields = []\n",
    "        for feature in self.feature_vector:\n",
    "            fields += [\"-e\", feature]\n",
    "\n",
    "        tshark_command = [\n",
    "            self.tshark_path,\n",
    "            \"-n\",                     # No DNS resolution (speeds up processing)\n",
    "            \"-r\", self.pcap_path,     # Input pcap file\n",
    "            \"-T\", \"fields\",           # Output in field format\n",
    "            *fields,                  # Include all requested fields\n",
    "            '-E', 'header=y',         # Add column headers\n",
    "            \"-E\", \"separator=\\t\",     # Use Tab as CSV delimiter\n",
    "            '-E', 'occurrence=f',     # First occurrence of repeated fields\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Run the tshark command and capture the output\n",
    "            result = subprocess.run(tshark_command, capture_output=True, text=True, check=True)\n",
    "            \n",
    "            # Convert the output to a list of lists\n",
    "            data = [line.split(\"\\t\") for line in result.stdout.strip().split(\"\\n\")]\n",
    "            \n",
    "            # Create a DataFrame from the data\n",
    "            self.dataframe = pd.DataFrame(data, columns=self.feature_vector)\n",
    "            return self.dataframe\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing tshark: {e}\")\n",
    "            print(\"Ensure that tshark is correctly installed and accessible from the specified path.\")\n",
    "            return None\n",
    "\n",
    "    def to_csv(self, output_file):\n",
    "        \"\"\"\n",
    "        Saves the extracted features DataFrame to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "            output_csv (str): Path to save the CSV file.\n",
    "        \"\"\"\n",
    "        fields = []\n",
    "        for feature in self.feature_vector:\n",
    "            fields += [\"-e\", feature]\n",
    "\n",
    "        tshark_command = [\n",
    "            self.tshark_path,\n",
    "            \"-n\",                     # No DNS resolution (speeds up processing)\n",
    "            \"-r\", self.pcap_path,     # Input pcap file\n",
    "            \"-T\", \"fields\",           # Output in field format\n",
    "            *fields,                  # Include all requested fields\n",
    "            '-E', 'header=y',         # Add column headers\n",
    "            \"-E\", \"separator=\\t\",     # Use Tab as CSV delimiter\n",
    "            '-E', 'occurrence=f',     # First occurrence of repeated fields\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'w') as out:\n",
    "                subprocess.run(tshark_command, stdout=out)\n",
    "\n",
    "            print(f\"tshark parsing complete. File saved as: {output_file}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing tshark: {e}\")\n",
    "            print(\"Ensure that tshark is correctly installed and accessible from the specified path.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fc6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "with open(\"../protocol_fields_output_missing_values.json\", \"r\") as file:\n",
    "    feature_config = json.load(file)\n",
    "\n",
    "features_to_extract = feature_config['features']\n",
    "features = [feature['field'] for feature in features_to_extract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6b7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_pcap_directory(input_dir, output_dir, features, is_malicious=False):\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pcap\"):\n",
    "                pcap_file = os.path.join(root, file)\n",
    "\n",
    "                # For malicious traffic, handle subfolder structure\n",
    "                if is_malicious:\n",
    "                    # Compute the relative path from the base directory (e.g., \"malicious_dir\")\n",
    "                    rel_dir = os.path.relpath(root, input_dir)\n",
    "    \n",
    "                    # Correct output directory path by appending the relative directory structure\n",
    "                    output_subdir = os.path.join(output_dir, rel_dir)\n",
    "                else:\n",
    "                    output_subdir = output_dir  # Normal traffic does not have subfolders\n",
    "\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "                # Set output file path in the correct subdirectory\n",
    "                output_file = os.path.join(output_subdir, file.replace(\".pcap\", \".csv\"))\n",
    "                \n",
    "                print(pcap_file)\n",
    "                pcapreader = PCAPReader(pcap_file, features, None, \"tshark\", None)\n",
    "                pcapreader.to_csv(output_file)\n",
    "\n",
    "\n",
    "# Input directories\n",
    "normal_dir = \"../data/raw/normal/\"\n",
    "malicious_dir = \"../data/raw/malicious/\"\n",
    "\n",
    "# Output directories\n",
    "output_dir_normal = \"../data/extracted_features/normal/\"\n",
    "output_dir_malicious = \"../data/extracted_features/malicious/\"\n",
    "\n",
    "# # Process normal traffic\n",
    "# process_pcap_directory(\n",
    "#     input_dir=normal_dir,\n",
    "#     output_dir=output_dir_normal,\n",
    "#     features=features,\n",
    "#     is_malicious=False\n",
    "# )\n",
    "\n",
    "# # Process malicious traffic\n",
    "# process_pcap_directory(\n",
    "#     input_dir=malicious_dir,\n",
    "#     output_dir=output_dir_malicious,\n",
    "#     features=features,\n",
    "#     is_malicious=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327016c",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984ed150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_variance(df, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Removes features from the DataFrame with variance lower than the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        threshold (float): The variance threshold. Features with variance below this value will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with low-variance features dropped.\n",
    "    \"\"\"\n",
    "    # Select numerical columns for variance calculation\n",
    "    numerical_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    # Initialize the VarianceThreshold selector\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "    # Fit the selector to the data and identify features to drop\n",
    "    selector.fit(numerical_df)\n",
    "    to_drop = numerical_df.columns[~selector.get_support()]\n",
    "\n",
    "    # Drop the low-variance features from the DataFrame\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "\n",
    "def replace_missing_values(df, num_replacement=-1, cat_replacement=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Replaces missing values in a DataFrame.\n",
    "    - Numerical columns: Replace with a specified value (default -1).\n",
    "    - Categorical columns: Replace with a specified value (default \"unknown\").\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with missing values.\n",
    "        num_replacement (int/float): Value to replace missing numerical values (default is -1).\n",
    "        cat_replacement (str): Value to replace missing categorical values (default is \"unknown\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values replaced.\n",
    "    \"\"\"\n",
    "    # Replace missing values in numerical columns with the specified replacement value\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[num_cols] = df[num_cols].fillna(num_replacement)\n",
    "\n",
    "    # Replace missing values in categorical columns with the specified replacement value\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(cat_replacement)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_high_correlation(df, correlation_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Removes features that have a correlation greater than the specified threshold with any other feature.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        correlation_threshold (float): The correlation threshold. Features with correlation above this value will be dropped.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with highly correlated features dropped.\n",
    "    \"\"\"\n",
    "    # Select numerical columns to calculate the correlation matrix\n",
    "    numerical_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    # Calculate the absolute correlation matrix\n",
    "    corr_matrix = numerical_df.corr().abs()\n",
    "\n",
    "    # Identify the upper triangle of the correlation matrix (excluding the diagonal)\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identify features to drop based on correlation threshold\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "\n",
    "    # Drop the highly correlated features from the DataFrame\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "\n",
    "def handle_missing_values(df, missing_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Removes columns from the DataFrame that have missing values above a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missing_threshold (float): The missing value threshold. Columns with a percentage of missing values greater than this will be dropped.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns containing excessive missing values dropped.\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = df.isnull().mean()\n",
    "\n",
    "    # Remove columns with missing percentage above the threshold\n",
    "    return df.loc[:, missing_percentage < missing_threshold]\n",
    "\n",
    "\n",
    "def clean_features(df):\n",
    "    \"\"\"\n",
    "    Full feature cleaning pipeline: applies multiple cleaning steps to the input DataFrame.\n",
    "    \n",
    "    The pipeline includes:\n",
    "    1. Handling missing values by removing columns with too many missing values.\n",
    "    2. Replacing missing values with specified replacements.\n",
    "    3. Removing low variance features.\n",
    "    4. Removing highly correlated features.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame after applying all cleaning steps.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove columns with excessive missing values\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # Step 2: Replace missing values in the DataFrame\n",
    "    df = replace_missing_values(df)\n",
    "    \n",
    "    # Step 3: Remove low-variance features\n",
    "    df = remove_low_variance(df)\n",
    "    \n",
    "    # Step 4: Remove highly correlated features\n",
    "    df = remove_high_correlation(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e3a11",
   "metadata": {},
   "source": [
    "## Federated Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493709d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_local_datasets(iot_devices):\n",
    "    \"\"\"\n",
    "    Process and clean datasets for a list of IoT devices by reading feature files,\n",
    "    cleaning them, and consolidating feature information for each device.\n",
    "\n",
    "    Parameters:\n",
    "        iot_devices (list): A list of IoT device identifiers to process.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sets representing the features for each device.\n",
    "    \"\"\"\n",
    "    feature_info = []\n",
    "\n",
    "    # Iterate over each IoT device\n",
    "    for iot_device in iot_devices:\n",
    "        # Get the list of file paths for normal and malicious data for the device\n",
    "        m_filenames = [f for f in glob.glob(f\"../data/extracted_features/malicious/*/{iot_device}_*.csv\") if 'mirai-dos' in f]\n",
    "        b_filenames = glob.glob(f\"../data/extracted_features/normal/{iot_device}_*.csv\")\n",
    "        b_filenames = []\n",
    "    \n",
    "        # Read and concatenate the chunks from all the files associated with the device\n",
    "        processed_chunks = []\n",
    "        for filename in b_filenames + m_filenames:\n",
    "            # Read each file in chunks to optimize memory usage\n",
    "            for chunk in pd.read_csv(filename, sep=\"\\t\", low_memory=False, chunksize=10000):\n",
    "                processed_chunks.append(chunk)\n",
    "\n",
    "        if processed_chunks == []:\n",
    "            continue\n",
    "            \n",
    "        # Concatenate all the chunks into a single DataFrame\n",
    "        df = pd.concat(processed_chunks)\n",
    "        \n",
    "        # Perform local cleaning (e.g., handling missing values, transforming features, etc.)\n",
    "        df_cleaned = clean_features(df)\n",
    "\n",
    "        # Track the columns (features) in the cleaned data for this IoT device\n",
    "        feature_info.append(set(df_cleaned.columns))\n",
    "\n",
    "        # Free up memory by deleting the DataFrames and forcing garbage collection\n",
    "        del df\n",
    "        del df_cleaned\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"IoT Device: {iot_device} Done!\")\n",
    "\n",
    "    return feature_info\n",
    "\n",
    "\n",
    "def federated_feature_consolidation(feature_info):\n",
    "    \"\"\"\n",
    "    Consolidate features across all devices by taking the union of features\n",
    "    from each device to ensure global consistency of features.\n",
    "\n",
    "    Parameters:\n",
    "        feature_info (list): A list of sets representing features for each IoT device.\n",
    "\n",
    "    Returns:\n",
    "        list: A consolidated list of global features that are common across all devices.\n",
    "    \"\"\"\n",
    "    # Intersect features across all devices to ensure global consistency\n",
    "    global_features = set.union(*feature_info)\n",
    "    return list(global_features)\n",
    "\n",
    "\n",
    "def apply_global_features(cleaned_data, global_features):\n",
    "    \"\"\"\n",
    "    Apply the global set of features to the cleaned data of each device.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_data (dict): A dictionary where keys are device names and values are DataFrames\n",
    "                             containing the cleaned feature data for each device.\n",
    "        global_features (list): A list of globally consistent feature names.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with cleaned data for each device, containing only the global features.\n",
    "    \"\"\"\n",
    "    for device, df_cleaned in cleaned_data.items():\n",
    "        # Select only the columns that are part of the global feature set\n",
    "        cleaned_data[device] = df_cleaned[global_features]\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def save_cleaned_data(cleaned_data, output_dir):\n",
    "    \"\"\"\n",
    "    Save the cleaned data of each IoT device to CSV files in the specified output directory.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_data (dict): A dictionary where keys are device names and values are DataFrames\n",
    "                             containing the cleaned feature data for each device.\n",
    "        output_dir (str): The directory where the cleaned data CSV files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save each device's cleaned data as a CSV file\n",
    "    for device, df in cleaned_data.items():\n",
    "        df.to_csv(os.path.join(output_dir, f\"{device}_cleaned.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6a07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_filenames = glob.glob(\"../data/extracted_features/normal/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc6fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/fpckxx395fs2zmxnmryyxw540000gp/T/ipykernel_15562/485290949.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[num_cols] = df[num_cols].fillna(num_replacement)\n",
      "/var/folders/2l/fpckxx395fs2zmxnmryyxw540000gp/T/ipykernel_15562/485290949.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[cat_cols] = df[cat_cols].fillna(cat_replacement)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoT Device: iotsim-ip-camera-street-1 Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/fpckxx395fs2zmxnmryyxw540000gp/T/ipykernel_15562/485290949.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[num_cols] = df[num_cols].fillna(num_replacement)\n",
      "/var/folders/2l/fpckxx395fs2zmxnmryyxw540000gp/T/ipykernel_15562/485290949.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[cat_cols] = df[cat_cols].fillna(cat_replacement)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoT Device: iotsim-air-quality-1 Done!\n"
     ]
    }
   ],
   "source": [
    "# Define paths to local datasets\n",
    "iot_devices = list(set([re.search(r\"([a-zA-Z\\-]+)-([0-9]+)\", f).group(0) for f in benign_filenames]))\n",
    "\n",
    "# Step 1: Process each device's dataset locally\n",
    "feature_info = process_local_datasets(iot_devices)\n",
    "\n",
    "# Step 2: Consolidate features across all devices\n",
    "m_global_features = federated_feature_consolidation(feature_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e63e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "# benign_global_features = list(set.union(*feature_info))\n",
    "# benign_global_features.sort()\n",
    "\n",
    "print(len(global_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf41b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_global_features = list(set.union(*feature_info))\n",
    "malicious_global_features.sort()\n",
    "\n",
    "print(len(malicious_global_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76861752",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95227925",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_features = list(set(malicious_global_features).union(set(benign_global_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84566349",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(filter(lambda x: x['field'] in global_features, feature_config['features']))\n",
    "res = {\"features\": res}\n",
    "with open(\"../protocol_fields_output_cleeaned.json\", \"w\") as file:\n",
    "    json.dump(res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb563c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
